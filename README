This repository is stored some implementations about variety of Boltzmann Machine, and relative training methods, including Parallel Tempering, Persistent 
Contrastive Divergence, Contrastive Divergence.
=======
Boltzmann Machine : --version=0.0

---INTRODUCTION---

This repository is stored some implementations about
variety of Boltzmann Machine, and relative training 
methods, including Parallel Tempering, Persistent 
Contrastive Divergence, Contrastive Divergence. 


The Cost functional we are using is Contrastive Divergence(difference between two KL divergence) 
is kind of variational method to deal with untraceable likelihood function.
The optimization we adopt the Stochastic Gradient Descent. 


I also include some important proof about the derivation about how or why the approximation works, and other MCMC method, the proof is in the RBM_note.pdf.

Please feel free to inform me if there are any mistakes in the proof or code. Thx.
The Testing Data including some standard data sets, and some data sets I had.
=======
The RBM is quite powerful to solving the Recommendation System problem. And I found PT
training can really improve the learning of a RBM, also have another benifits(AIS for esitmate the 
partition part to monitor the learning of RBM).

---Respository Description---

code:
	Including the basic outline of the basic algorithms, which just give a quick implementation.
	Current Models:
		Restricted Boltzmann Machine with Bernoulli Unit
		Restricted Boltzmann Machine with Multi-Class Bernoulli(multivariate bernoulli)
		Recurrent Temporal Restricted Boltzmann Machine with Bernoulli
		Recurrent Temporal Restricted Boltzmann Machine with Multi-Class Bernoulli(multivariate bernoulli)
		Parallel Tempering Trained Restricted Boltzmann Machine with Bernoulli Unit
	Upcomming Model:
		Semi-Restricted Boltzmann Machine with Bernolli Unit
		Random Markov Field(No hidden unit RBM but visible unit are fully connected).
		

Reference:
	Deep Tempering	
	Recurrent Temporal RBM 
	RBM
	AdaDelta
	Multi-Class RBM

Future works:
	Modulize the Repository.
	Connecting the MEGAMA(http://icl.cs.utk.edu/magma/)to improve the performance of some operation.
	Trying to improve the Theano, and learn form it.
